{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully Connected Neural Networks in TensorFlow's Keras\n",
    "\n",
    "We will use dense neural networks in Keras to solve a simple regression problem. \n",
    "\n",
    "The dataset is a collection of simulated particle events from [Pythia](http://home.thep.lu.se/~torbjorn/Pythia.html) and is provided on our GitHub page. \n",
    "\n",
    "Specifically, you will construct a dense neural network which will learn to calculate the invariant mass of a particle from its energy, momentum, and charge.\n",
    "\n",
    "First, import `numpy`, `tensorflow`, and `pylab` and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab as plt\n",
    "\n",
    "# Prevent TensorFlow from showing us deprecation warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# MPK comp hack. You should not need this.\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load the data, which is stored as a numpy array data type\n",
    "data = np.load(\"../data/homogenous-16-particle-events-energy.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem: $e^- + p$ collisions\n",
    "\n",
    "<img src=\"https://www.jlab.org/sites/default/files/images/news/releases/Hen_2.jpg\" width=\"50%\" align=\"middle\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Datset: electron-proton collision\n",
    "   * 2D array\n",
    "   * row = one event\n",
    "   * ONLY events where exactly 16 particles produced\n",
    "   * each particle given by $(p_x,p_y,p_z,E,q)$\n",
    "   * each event = 80 floats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35916, 80)\n",
      "(35916, 16, 5)\n",
      "(574656, 5)\n"
     ]
    }
   ],
   "source": [
    "#print(data)\n",
    "print(data.shape)\n",
    "# isolate particles:\n",
    "data = np.reshape(data,(len(data),16,5))\n",
    "print(data.shape)\n",
    "#Use another call of reshape to combine all events\n",
    "data = np.reshape(data,(len(data)*16,5))\n",
    "\n",
    "#Try printing the energy to make sure this worked\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are our training data inputs, but we also must provide the targets, which are the invariant masses of each particle. This is a straightforward computation that does not require neural networks, but provides an easy-to-understand problem.\n",
    "\n",
    "We choose units where $c = 1$:\n",
    "\n",
    "$$m^2=E^2-||\\textbf{p}||^2$$\n",
    "\n",
    "where $m, E$, and $\\textbf{p}$ are all in GeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Note: There may be roundoff error, so consider that before taking the squareroot. \n",
    "\n",
    "p2 = data[:,0]**2 + data[:,1]**2 + data[:,2]**2\n",
    "mass = np.sqrt(np.maximum(data[:,3]**2 - p2,0))\n",
    "#print(mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's make sure that we are seeing reasonable masses of particles. As this data has limited precision, this will not resolve electrons very well, but protons, pions, and massless particles should be clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(mass,bins=100)\n",
    "plt.xlabel(\"invariant mass (GeV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are now over 500k training examples, which is overkill for this simple example. Let's use 1000 examples.\n",
    "\n",
    "We know from a physics standpoint that charge is not required to calculate invariant mass. Should we remove it from our training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.492933  , -0.469448  , -2.22871   ,  2.33452   , -1.        ],\n",
       "       [-0.380699  ,  0.859243  ,  0.125686  ,  0.958388  , -1.        ],\n",
       "       [-0.234261  ,  0.933082  ,  1.29211   ,  1.61695   , -1.        ],\n",
       "       ...,\n",
       "       [-0.0741433 ,  0.12017   , -0.33218   ,  0.360945  ,  0.        ],\n",
       "       [-0.0368308 ,  0.0430933 , -0.0116498 ,  0.0578728 ,  0.        ],\n",
       "       [-0.0262733 ,  0.00345447,  0.350706  ,  0.351706  ,  0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing allows you select a subset of an array.\n",
    "\n",
    "data_train = data[:1000]\n",
    "target = mass[:1000]\n",
    "data_train.shape\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have our data as we want to present it to our model, we can build and train our first neural network. \n",
    "\n",
    "<https://www.tensorflow.org/api_docs/python/tf/keras>.\n",
    "\n",
    "## Build model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# model with linear stacking of layers\n",
    "model = tf.keras.Sequential()\n",
    "# add one hidden layer:\n",
    "model.add(tf.keras.layers.Dense(5,input_shape=(5,), activation=\"relu\")) \n",
    "# output layer: no activation function for regression\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(5,)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define our training configuration:\n",
    "\n",
    "`compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define training configuration:\n",
    "# Adam optimizer and mean squared error loss\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.1), loss=tf.keras.losses.MeanSquaredError()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train model:\n",
    "\n",
    "`fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "results = model.fit(data_train, target, epochs=30, batch_size=32, validation_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "history = results.history\n",
    "plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is learning, but we can tune the model for better results. \n",
    "\n",
    "Perhaps we did not have enough fitting parameters to accurately represent the mapping. Remedy this by increasing the number of hidden neurons to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same code as in the previous cell\n",
    "#simply change the number of neurons in the hidden layer\n",
    "\n",
    "res = model.evaluate(data[-10:],mass[-10:])\n",
    "\n",
    "print(mass[-10:])\n",
    "y = model.predict(data[-10:])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we got little improvement here. Another hyperparameter to adjust is *batch size*, which is the number of training examples used to calculate the gradient at each iteration. While you may initially think that a higher batch size leads to faster or more accurate training, in practice this is not true. The \"noise\" that arises from using less training examples at each iteration can actually help find the global minimum of the loss function.\n",
    "(See here for more info: https://arxiv.org/pdf/1609.04836.pdf)\n",
    "\n",
    "Try decreasing the batch size to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "results = model.fit(data_train, target, epochs=30, batch_size=16, validation_split=0.8, verbose=0)\n",
    "history = results.history\n",
    "plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is starting to do better but has significant room for improvemnt. \n",
    "\n",
    "Another hyperparameter to tune is the *learning rate*. \n",
    "\n",
    " - If the learning rate is too high, we are taking too large of a step in the gradient descent at each iteration and will miss narrow minima in the loss function. \n",
    " - If the learning rate is too small, then we are not traveling far enough in each iteration and we will take far too long to reach a minimum. \n",
    "\n",
    "Perhpas the learning rate is to high and the network can't fine tune. Try decreasing the learning rate to 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(data_train, target, epochs=30, batch_size=16, validation_split=0.8, verbose=0)\n",
    "history = results.history\n",
    "plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not really that mutch better, but now there is evidence of *overtraining* or *overfitting* -- the training loss is so much lower than the validation loss. \n",
    "\n",
    "A common fix to this is adding *dropout layers*. Try adding a dropout layer with dropout rate of 0.5. <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout layers are located under tf.keras.layers. \n",
    "#They take the dropout rate as their only argument.\n",
    "\n",
    "#Complete me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly stopped the overtraining problem, but it still isn't training well. Now, try training on the full dataset with a more reasonable validation split of 0.2. Use the a single hidden layer with 20 neurons, a learning rate of 0.001, and a batch size of 256. Just run it for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly resulted in a significant improvement and shows how important having a large enough dataset it. Moving on to the choice in activation functions, ReLU is not the only available choice, although it is one of the most popular ones currently. Try training a network using a sigmoid or tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simply change relu to sigmoid or tanh to change the activation function\n",
    "\n",
    "#Complete me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try adding 2 new hidden layers to the network. Use the relu activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, adding more layers helps improve the quality of the network. There is a limit to now effective this is though. Try having 5 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see what happens when you increase the number of neurons per layer from 20 to 50 in the 3 hidden layer model. Consider how they perform compared to Relu now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the sigmoid and the tanh activation functions again and compare them to relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference in performace, especially with the sigmoid function, is known as the vanishing gradient problem. If the value for any one the neurons gets too far away from 0, the gradient for sigmoid and tanh gets really close to 0. This means that for deeper networks it is much more difficult to update the first layers as their gradient is so small. Now, remove the fifth column from the input data, the charge, and see what happens when training. Why do you thing including charge has such a large impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there are other options for the loss function. Try experimenting with mean squared error. Other optimizers that can be used are sgd, rmsprop, adagrad, adadelta, adamax, and nadam. <https://www.tensorflow.org/api_docs/python/tf/keras/optimizers>\n",
    "\n",
    "<https://www.tensorflow.org/api_docs/python/tf/keras/losses>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
